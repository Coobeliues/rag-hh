# RAG по вакансиям hh.kz

Система семантического поиска и анализа IT-вакансий Казахстана с использованием RAG (Retrieval-Augmented Generation).

## Описание для резюме

> **RAG-система по вакансиям hh.kz** — pet-проект, демонстрирующий навыки работы с NLP-пайплайном полного цикла. Спарсил 1 278 IT-вакансий через открытый API hh.ru, построил векторный индекс (FAISS, 1 934 чанка) с мультиязычными эмбеддингами (multilingual-e5-small), реализовал RAG-пайплайн с подключением LLM (Ollama/OpenAI) для генерации ответов на естественном языке. Интерфейс — Streamlit-приложение с фильтрами (город, зарплата, опыт), карточками вакансий и аналитическим дашбордом. Покрытие тестами — 49 тестов (pytest).
>
> **Стек:** Python, FAISS, Sentence-Transformers, Streamlit, Ollama, REST API, BeautifulSoup, Pandas

---

## Как это всё работает (подробное объяснение)

### Архитектура проекта

```
rag_hh/
├── config.py                 # Конфигурация (API URLs, коды регионов)
├── parse.py                  # CLI для парсинга вакансий
├── merge_data.py             # Объединение и дедупликация данных
├── build_index.py            # Построение FAISS-индекса
├── app.py                    # Streamlit UI (веб-интерфейс)
│
├── parser/                   # Модуль парсинга
│   ├── hh_parser.py          # Работа с API hh.ru
│   └── storage.py            # Сохранение в JSON/CSV
│
├── rag/                      # Модуль RAG
│   ├── chunker.py            # Нарезка вакансий на чанки
│   ├── indexer.py            # FAISS индекс + поиск + фильтры
│   └── pipeline.py           # RAG-пайплайн (поиск → LLM → ответ)
│
├── tests/                    # Тесты (pytest)
│   ├── conftest.py           # Фикстуры
│   ├── test_chunker.py       # Тесты чанкера
│   ├── test_indexer.py       # Тесты фильтров
│   ├── test_parser.py        # Тесты парсера
│   └── test_pipeline.py      # Тесты пайплайна
│
└── data/                     # Данные
    ├── *_kz.json/csv         # Спарсенные вакансии по категориям
    ├── vacancies_all.json    # Объединённый датасет (1 278 вакансий)
    └── index/                # FAISS индекс
        ├── vacancies.index   # Бинарный файл индекса
        ├── chunks.pkl        # Чанки с метаданными (pickle)
        └── config.json       # Конфиг модели и индекса
```

### Этап 1: Парсинг вакансий (`parser/`)

**Что происходит:** Скрипт ходит в открытый API hh.ru и собирает вакансии.

**Как работает API hh.ru:**
- Эндпоинт `GET https://api.hh.ru/vacancies?text=Python&area=160` — поиск вакансий
- Возвращает JSON со списком вакансий (до 100 за запрос, макс 2000 через пагинацию)
- Каждая вакансия в списке — краткая (название, компания, зарплата, город)
- Для полного описания нужен отдельный запрос: `GET /vacancies/{id}` — тут уже есть полное описание, ключевые навыки, требования к опыту
- Авторизация не нужна, но есть rate limit ~5 запросов/сек

**Процесс:**
1. `fetch_vacancy_ids()` — ищет вакансии по запросу, собирает ID и краткие данные
2. `fetch_vacancy_detail()` — для каждой вакансии получает полное описание (с задержкой 0.3 сек, чтобы не забанили)
3. `parse_vacancy()` — объединяет краткие данные + детали в плоский dict
4. `clean_html()` — описания приходят в HTML, BeautifulSoup убирает теги
5. `parse_salary()` — превращает вложенный dict зарплаты в плоские поля

**Пример данных одной вакансии после парсинга:**
```json
{
  "id": "12345",
  "name": "Python Developer",
  "employer_name": "Kaspi.kz",
  "area": "Алматы",
  "salary_from": 500000,
  "salary_to": 800000,
  "salary_currency": "KZT",
  "experience": "От 1 до 3 лет",
  "key_skills": "Python, Django, PostgreSQL, REST API",
  "description": "Мы ищем опытного Python-разработчика...",
  "url": "https://hh.kz/vacancy/12345"
}
```

**Что собрано:** 11 категорий (Python, Data Science, Backend, ML, Java, Frontend, DevOps, QA, Аналитик, Сисадмин, 1С) → после дедупликации **1 278 уникальных вакансий** от **659 компаний**.

---

### Этап 2: Чанкинг (`rag/chunker.py`)

**Зачем:** Эмбеддинг-модели имеют ограничение на длину текста (~512 токенов). Длинные вакансии нужно разбить на части.

**Как работает:**
1. `vacancy_to_document()` — превращает dict вакансии в структурированный текст на русском:
   ```
   Вакансия: Python Developer
   Компания: Kaspi.kz
   Город: Алматы
   Зарплата: от 500000 до 800000 KZT
   Опыт: От 1 до 3 лет
   Ключевые навыки: Python, Django, PostgreSQL

   Описание:
   Мы ищем опытного Python-разработчика...
   ```

2. `chunk_documents()` — если текст ≤ 1500 символов → один чанк. Если длиннее → разрезается на куски по 1500 символов с перехлёстом 200 символов (чтобы не терять контекст на границах).

3. Каждый чанк хранит **метаданные** — vacancy_id, название, компания, город, зарплата, опыт, URL. Это нужно для фильтрации и отображения.

**Результат:** 1 278 вакансий → **1 934 чанка** (некоторые длинные вакансии разбились на 2-3 чанка).

---

### Этап 3: Эмбеддинги и FAISS-индекс (`rag/indexer.py`)

**Зачем:** Чтобы искать вакансии по смыслу, а не по ключевым словам.

**Эмбеддинги — что это:**
- Модель `intfloat/multilingual-e5-small` превращает текст в вектор из 384 чисел
- Похожие по смыслу тексты → похожие векторы (близко друг к другу в пространстве)
- "Python разработчик" и "Python developer" будут иметь похожие векторы, хотя слова разные
- Модель мультиязычная — понимает русский, английский, казахский

**E5-модель — особенность:**
- Документы кодируются с префиксом `"passage: "` перед текстом
- Запросы кодируются с префиксом `"query: "` перед текстом
- Это повышает качество поиска (модель обучена различать запросы и документы)

**FAISS (Facebook AI Similarity Search):**
- Библиотека от Meta для быстрого поиска ближайших векторов
- `IndexFlatIP` — точный поиск по Inner Product (скалярное произведение)
- Так как векторы нормализованы (длина = 1), Inner Product = косинусное сходство
- Поиск по 1 934 векторам занимает < 1 мс

**Процесс построения:**
1. Загружаем модель `multilingual-e5-small`
2. К каждому чанку добавляем префикс `"passage: "`
3. Кодируем все 1 934 чанка → получаем матрицу 1934 × 384
4. Нормализуем векторы (для косинусного сходства)
5. Добавляем в FAISS-индекс
6. Сохраняем на диск: `vacancies.index` (FAISS), `chunks.pkl` (метаданные), `config.json`

**Поиск с фильтрами:**
1. Запрос пользователя кодируется с префиксом `"query: "` → вектор
2. FAISS ищет top-K ближайших чанков по косинусному сходству
3. Если есть фильтры (город, зарплата, опыт) — берём 5× больше кандидатов, потом отсеиваем по метаданным
4. Возвращаем отфильтрованные результаты со скорами (0.0–1.0)

---

### Этап 4: RAG-пайплайн (`rag/pipeline.py`)

**RAG = Retrieval-Augmented Generation** — паттерн, когда LLM отвечает не из своей "головы", а на основе найденных документов.

**Зачем:** LLM не знает про конкретные вакансии на hh.kz. Мы сначала ищем релевантные вакансии, затем подставляем их в промпт как контекст.

**Пайплайн (что происходит при запросе):**

```
Пользователь: "Какие компании в Алматы ищут Python-разработчиков?"
                    │
                    ▼
    ┌─────────────────────────────┐
    │ 1. FAISS-поиск (indexer.py) │  → Находит 10 самых релевантных чанков
    └─────────────────────────────┘
                    │
                    ▼
    ┌─────────────────────────────┐
    │ 2. Формирование контекста   │  → Дедупликация по vacancy_id
    │    (format_context)         │  → Макс 8 вакансий в контекст
    └─────────────────────────────┘
                    │
                    ▼
    ┌─────────────────────────────┐
    │ 3. Составление промпта      │  → Системный промпт + вакансии + вопрос
    └─────────────────────────────┘
                    │
                    ▼
    ┌─────────────────────────────┐
    │ 4. LLM генерирует ответ     │  → Ollama (qwen2.5:3b) или OpenAI
    └─────────────────────────────┘
                    │
                    ▼
    "В Алматы Python-разработчиков ищут:
     1. Kaspi.kz — от 500K KZT, требуют Django + PostgreSQL
     2. Kolesa Group — от 700K KZT, микросервисы на FastAPI
     ..."
```

**Промпт для LLM:**
- **Системный промпт:** "Ты — ассистент по вакансиям hh.kz. Отвечай ТОЛЬКО на основе данных. Если нет ответа — скажи честно."
- **Пользовательский промпт:** Список найденных вакансий + вопрос пользователя + инструкция приводить конкретные примеры

**LLM-бэкенды:**
- **Ollama** — локальная модель `qwen2.5:3b` (бесплатно, работает на CPU, ~30 сек на ответ)
- **OpenAI** — `gpt-4o-mini` через API (платно, быстрее, качественнее)
- **none** — без LLM, только семантический поиск

---

### Этап 5: Streamlit UI (`app.py`)

**Streamlit** — Python-фреймворк для быстрого создания веб-интерфейсов. Пишешь обычный Python-код → получаешь интерактивный веб-сайт.

**Две вкладки:**

#### Вкладка "Поиск"
- Текстовое поле для запроса на естественном языке
- Кнопки с примерами запросов
- Ответ AI (если подключён LLM)
- Карточки вакансий с процентом релевантности, названием, компанией, городом, зарплатой
- Кнопка "Открыть на hh.kz" для перехода к оригиналу

#### Вкладка "Аналитика"
- Распределение вакансий по городам (bar chart)
- Топ-15 компаний по количеству вакансий
- Анализ зарплат (мин / медиана / макс в KZT)
- Топ-25 ключевых навыков

#### Сайдбар (фильтры)
- **Город** — selectbox из реальных городов в базе
- **Мин. зарплата** — number input (шаг 50K KZT)
- **Опыт** — Нет опыта / 1-3 года / 3-6 лет / 6+ лет
- **LLM** — выбор бэкенда (none / ollama / openai)
- **Статистика** — сколько вакансий, компаний, городов в базе

**Кеширование:**
- `@st.cache_resource` — индекс и модель загружаются один раз при старте
- `@st.cache_data` — метаданные (города, компании) считаются один раз

---

### Как всё связано (полный цикл)

```
[hh.ru API] → parse.py → JSON-файлы → merge_data.py → vacancies_all.json
                                                              │
                                                              ▼
                                                       build_index.py
                                                              │
                                           chunker.py ← ──────┤
                                           (текст + метаданные)│
                                                              ▼
                                           indexer.py → FAISS index + chunks.pkl
                                                              │
                                                              ▼
                                              app.py (Streamlit) ←── pipeline.py
                                                │                        │
                                          пользователь              Ollama/OpenAI
```

---

## Быстрый старт

```bash
# 1. Создать виртуальное окружение
python3 -m venv venv
source venv/bin/activate

# 2. Установить зависимости
pip install -r requirements.txt

# 3. Спарсить вакансии
python parse.py -q "Python developer" -a almaty --json-out data/python_almaty.json
python parse.py -q "Data Science" -a kazakhstan --json-out data/ds_kz.json

# 4. Объединить данные
python merge_data.py

# 5. Построить индекс
python build_index.py

# 6. (Опционально) Установить Ollama + модель
# curl -fsSL https://ollama.com/install.sh | sh
# ollama pull qwen2.5:3b

# 7. Запустить приложение
streamlit run app.py
```

## Запуск тестов

```bash
pytest tests/ -v
```

## Стек технологий

| Компонент | Технология |
|-----------|------------|
| Парсинг | requests, BeautifulSoup, hh.ru REST API |
| Эмбеддинги | sentence-transformers, intfloat/multilingual-e5-small |
| Векторный поиск | FAISS (IndexFlatIP, косинусное сходство) |
| LLM | Ollama (qwen2.5:3b) / OpenAI (gpt-4o-mini) |
| Интерфейс | Streamlit |
| Данные | Pandas, JSON |
| Тесты | pytest (49 тестов) |

## Статистика проекта

- **1 278** вакансий, **659** компаний
- **1 934** чанка в индексе (384-dim vectors)
- **~1 600** строк Python-кода
- **49** тестов (pytest)
- **11** категорий: Python, Data Science, Backend, ML, Java, Frontend, DevOps, QA, Аналитик, Сисадмин, 1С
