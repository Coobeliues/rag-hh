#!/usr/bin/env python3


import streamlit as st
import pandas as pd
from collections import Counter
from rag.indexer import load_index, search
from rag.pipeline import rag_query


# --- Page config ---
st.set_page_config(
    page_title="RAG –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º hh.kz",
    page_icon="üîç",
    layout="wide",
)

st.title("üîç RAG –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º hh.kz")
st.caption("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ IT-–≤–∞–∫–∞–Ω—Å–∏–π –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞")


# --- Load index (cached) ---
@st.cache_resource
def get_index():


    return load_index()


try:
    index, model, chunks = get_index()
except Exception as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å: {e}")
    st.info("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: `python build_index.py`")
    st.stop()


# --- Precompute metadata for filters ---
@st.cache_data
def get_metadata(_chunks):
    cities = sorted(set(c.get("area", "") for c in _chunks if c.get("area")))
    companies = sorted(set(c.get("employer", "") for c in _chunks if c.get("employer")))
    unique_vacancies = len(set(c["vacancy_id"] for c in _chunks))


    return cities, companies, unique_vacancies


cities, companies, n_vacancies = get_metadata(chunks)


def _format_salary(r: dict) -> str:
    """Format salary for display."""
    parts = []
    if r.get("salary_from"):
        parts.append(f"–æ—Ç {r['salary_from']:,}")
    if r.get("salary_to"):
        parts.append(f"–¥–æ {r['salary_to']:,}")
    s = " ".join(parts)
    if s and r.get("salary_currency"):
        s += f" {r['salary_currency']}"


    return s


# ======= SIDEBAR =========
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")

top_k = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", 3, 30, 10)

# --- Filters ---
st.sidebar.subheader("–§–∏–ª—å—Ç—Ä—ã")

filter_city = st.sidebar.selectbox("–ì–æ—Ä–æ–¥", ["–í—Å–µ"] + cities)
filter_salary = st.sidebar.number_input(
    "–ú–∏–Ω. –∑–∞—Ä–ø–ª–∞—Ç–∞ (KZT)", min_value=0, max_value=5_000_000, value=0, step=50_000,
    help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π –Ω–µ –Ω–∏–∂–µ —ç—Ç–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"
)
filter_experience = st.sidebar.selectbox(
    "–û–ø—ã—Ç",
    ["–õ—é–±–æ–π", "–ù–µ—Ç –æ–ø—ã—Ç–∞", "–û—Ç 1 –¥–æ 3 –ª–µ—Ç", "–û—Ç 3 –¥–æ 6 –ª–µ—Ç", "–ë–æ–ª–µ–µ 6 –ª–µ—Ç"],
)

# Build filters dict
filters = {}
if filter_city != "–í—Å–µ":
    filters["city"] = filter_city
if filter_salary > 0:
    filters["salary_min"] = filter_salary
if filter_experience != "–õ—é–±–æ–π":
    filters["experience"] = filter_experience

# --- LLM ---
st.sidebar.markdown("---")
st.sidebar.subheader("ü§ñ LLM")

llm_backend = st.sidebar.selectbox(
    "Backend",
    ["none", "ollama", "openai"],
    help="none = —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞",
)

llm_model = ""
api_key = ""
if llm_backend == "ollama":
    llm_model = st.sidebar.text_input("Ollama model", "qwen2.5:3b")
elif llm_backend == "openai":
    llm_model = st.sidebar.text_input("OpenAI model", "gpt-4o-mini")
    api_key = st.sidebar.text_input("OpenAI API Key", type="password")

# --- Stats ---
st.sidebar.markdown("---")
st.sidebar.subheader("üìä –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö")
col1, col2 = st.sidebar.columns(2)
col1.metric("–í–∞–∫–∞–Ω—Å–∏–π", n_vacancies)
col2.metric("–ö–æ–º–ø–∞–Ω–∏–π", len(companies))
st.sidebar.metric("–ì–æ—Ä–æ–¥–æ–≤", len(cities))
st.sidebar.metric("–ß–∞–Ω–∫–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ", index.ntotal)


# ======== MAIN AREA ==========

# --- Tabs ---
tab_search, tab_analytics = st.tabs(["üîç –ü–æ–∏—Å–∫", "üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞"])

# ========== TAB: SEARCH =========
with tab_search:
    # Example queries
    examples = [
        "Python-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –≤ –ê–ª–º–∞—Ç—ã",
        "–ù–∞–≤—ã–∫–∏ –¥–ª—è Data Science",
        "Backend —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π –æ—Ç 500K",
        "–°—Ä–∞–≤–Ω–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è ML-–≤–∞–∫–∞–Ω—Å–∏–π",
    ]
    cols = st.columns(len(examples))
    for col, ex in zip(cols, examples):
        if col.button(ex, use_container_width=True):
            st.session_state["query"] = ex

    query = st.text_input(
        "–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º:",
        value=st.session_state.get("query", ""),
        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –ö–∞–∫–∏–µ –Ω–∞–≤—ã–∫–∏ —á–∞—â–µ –≤—Å–µ–≥–æ —Ç—Ä–µ–±—É—é—Ç –¥–ª—è Backend?",
    )

    if query:
        with st.spinner("–ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏..."):
            results = search(query, index, model, chunks, top_k=top_k, filters=filters if filters else None)

        if not results:
            st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏–ª–∏ –∑–∞–ø—Ä–æ—Å.")
        else:
            # --- LLM answer ---
            if llm_backend != "none":
                with st.spinner("ü§ñ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç..."):
                    try:
                        kwargs = {}
                        if llm_backend == "openai" and api_key:
                            kwargs["api_key"] = api_key
                        response = rag_query(
                            query, index, model, chunks,
                            llm_backend=llm_backend,
                            llm_model=llm_model,
                            top_k=top_k,
                            **kwargs,
                        )
                        st.markdown("### ü§ñ –û—Ç–≤–µ—Ç AI")
                        st.info(response["answer"])
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ LLM: {e}")

            # --- Results header --
            # Deduplicate
            seen = set()
            unique_results = []
            for r in results:
                vid = r["vacancy_id"]
                if vid not in seen:
                    seen.add(vid)
                    unique_results.append(r)

            st.markdown(f"### –ù–∞–π–¥–µ–Ω–æ: {len(unique_results)} –≤–∞–∫–∞–Ω—Å–∏–π")
            if filters:
                active = []
                if filters.get("city"):
                    active.append(f"–≥–æ—Ä–æ–¥: {filters['city']}")
                if filters.get("salary_min"):
                    active.append(f"–∑–∞—Ä–ø–ª–∞—Ç–∞ ‚â• {filters['salary_min']:,} KZT")
                if filters.get("experience"):
                    active.append(f"–æ–ø—ã—Ç: {filters['experience']}")
                st.caption(f"–§–∏–ª—å—Ç—Ä—ã: {' | '.join(active)}")

            # --- Vacancy cards ---
            for r in unique_results:
                sal_str = _format_salary(r)
                score_pct = int(r["score"] * 100)

                with st.container(border=True):
                    c1, c2 = st.columns([4, 1])
                    with c1:
                        st.markdown(f"#### {r['vacancy_name']}")
                        st.markdown(f"üè¢ **{r['employer']}** &nbsp;|&nbsp; üìç {r['area']}"
                                    + (f" &nbsp;|&nbsp; üí∞ {sal_str}" if sal_str else ""))
                    with c2:
                        st.metric("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", f"{score_pct}%")


                    with st.expander("–ü–æ–¥—Ä–æ–±–Ω–µ–µ"):
                        st.markdown(r["text"])
                        url = r.get("url", "")
                        if url:
                            st.link_button("–û—Ç–∫—Ä—ã—Ç—å –Ω–∞ hh.kz", url)


#========== TAB: ANALYTICS ========
with tab_analytics:
    st.markdown("### üìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –±–∞–∑–µ –≤–∞–∫–∞–Ω—Å–∏–π")

    # City distribution
    city_counts = Counter(c.get("area", "–ù–µ —É–∫–∞–∑–∞–Ω") for c in chunks if c.get("chunk_index", 0) == 0)
    col_a, col_b = st.columns(2)

    with col_a:
        st.markdown("**–í–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –≥–æ—Ä–æ–¥–∞–º**")
        st.bar_chart({k: v for k, v in city_counts.most_common(15)})

    # Companies with most vacancies
    company_counts = Counter(c.get("employer", "") for c in chunks if c.get("chunk_index", 0) == 0)
    with col_b:
        st.markdown("**–¢–æ–ø-15 –∫–æ–º–ø–∞–Ω–∏–π**")
        st.bar_chart({k: v for k, v in company_counts.most_common(15)})

    # Salary analysis
    st.markdown("---")
    st.markdown("**–ê–Ω–∞–ª–∏–∑ –∑–∞—Ä–ø–ª–∞—Ç**")
    salaries = []
    for c in chunks:
        if c.get("chunk_index", 0) != 0:
            continue
        sal_from = c.get("salary_from") or 0
        sal_to = c.get("salary_to") or 0
        best = max(sal_from, sal_to)
        if best > 0:
            salaries.append({
                "vacancy": c.get("vacancy_name", ""),
                "company": c.get("employer", ""),
                "salary": best,
                "currency": c.get("salary_currency", ""),
            })

    if salaries:
        df_sal = pd.DataFrame(salaries)
        kzt = df_sal[df_sal["currency"] == "KZT"]
        if not kzt.empty:
            col_s1, col_s2, col_s3 = st.columns(3)
            col_s1.metric("–ú–∏–Ω. –∑–∞—Ä–ø–ª–∞—Ç–∞ (KZT)", f"{int(kzt['salary'].min()):,}")
            col_s2.metric("–ú–µ–¥–∏–∞–Ω–∞ (KZT)", f"{int(kzt['salary'].median()):,}")
            col_s3.metric("–ú–∞–∫—Å. –∑–∞—Ä–ø–ª–∞—Ç–∞ (KZT)", f"{int(kzt['salary'].max()):,}")

            st.bar_chart(kzt["salary"].value_counts().sort_index())

        st.markdown(f"–í–∞–∫–∞–Ω—Å–∏–π —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π: **{len(salaries)}** –∏–∑ {n_vacancies} ({100 * len(salaries) // max(n_vacancies, 1)}%)")
    else:
        st.info("–ù–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–π —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π.")


    # Skills word cloud (text-based   
    st.markdown("---")
    st.markdown("**–¢–æ–ø –Ω–∞–≤—ã–∫–æ–≤ (key_skills)**")
    all_skills = []
    for c in chunks:
        text = c.get("text", "")
        try:
            if "–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏:" in text:
                skills_line = text.split("–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏:")[1].split("\n")[0].strip()
                for skill in skills_line.split(","):
                    s = skill.strip()
                    if s:
                        all_skills.append(s)
        except (IndexError, ValueError):
            continue

    if all_skills:
        skill_counts = Counter(all_skills)
        df_skills = pd.DataFrame(skill_counts.most_common(25), columns=["–ù–∞–≤—ã–∫", "–ö–æ–ª-–≤–æ"])
        st.dataframe(df_skills, width="stretch", hide_index=True)
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –Ω–∞–≤—ã–∫–∞–º.")
